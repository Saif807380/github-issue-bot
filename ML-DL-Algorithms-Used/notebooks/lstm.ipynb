{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbU720xZDLud",
    "outputId": "3f7fbe26-03d6-40c5-af42-df71f1806af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9UzCeqG9DRHL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eApWJu8kDk2V"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4uguSm9wDnVS"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/github-issue-bot/saif_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "omaYXKdIDp0f"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for idx, data in df.iterrows():\n",
    "  counter.update(tokenizer(data[\"issue\"]))\n",
    "counter.update([\"_PAD\", \"_UNK\"])\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iRUNprIZECe3"
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "6uY_Pp9rEQUE"
   },
   "outputs": [],
   "source": [
    "dataset = df[[\"issue\", \"label\"]].copy(deep=True)\n",
    "dataset[\"issue\"] = dataset[\"issue\"].apply(lambda x: np.str_(x))\n",
    "dataset[\"issue\"] = dataset[\"issue\"].apply(lambda x: text_pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "M_iriKWsVFAZ"
   },
   "outputs": [],
   "source": [
    "def pad_input(sentences, seq_len=300):\n",
    "  features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "  for ii, review in enumerate(sentences):\n",
    "    if len(review) != 0:\n",
    "      features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "yKo-fsuMEeQh"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.15)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.05)\n",
    "train_data.reset_index(inplace=True, drop=True)\n",
    "valid_data.reset_index(inplace=True, drop=True)\n",
    "test_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "FZkVnFh-ElRl"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, n_layers=1):\n",
    "    super(LSTMClassifier, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_dim, num_class)\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.n_layers = n_layers\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "    initrange = 0.5\n",
    "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "    self.fc.bias.data.zero_()\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    x = self.dropout(x)\n",
    "    lstm_out, (ht, ct) = self.lstm(x)\n",
    "    out = self.fc(ht[-1])\n",
    "    return out\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "OVYRgkyoKg-8"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, criterion, h):\n",
    "  model.train()\n",
    "  total_acc, total_count = 0, 0\n",
    "  log_interval = 50\n",
    "  start_time = time.time()\n",
    "\n",
    "  for idx, (text, label) in enumerate(dataloader):\n",
    "    h = tuple([e.data for e in h])\n",
    "    # forward prop\n",
    "    predicted_label = model(text, h)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(predicted_label, label)\n",
    "\n",
    "    # backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "    total_count += label.size(0)\n",
    "\n",
    "\n",
    "    if idx % log_interval == 0 and idx > 0:\n",
    "      elapsed = time.time() - start_time\n",
    "      print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "            '| accuracy {:8.3f} | time {:5.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                        total_acc/total_count, elapsed))\n",
    "      total_acc, total_count = 0, 0\n",
    "      start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader, criterion, h):\n",
    "  model.eval()\n",
    "  total_acc, total_count = 0, 0\n",
    "\n",
    "  # don't calculate backward gradients\n",
    "  with torch.no_grad():\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "      predicted_label = model(text, h)\n",
    "      loss = criterion(predicted_label, label)\n",
    "      total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "      total_count += label.size(0)\n",
    "  return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "I96hb5bfV2x6"
   },
   "outputs": [],
   "source": [
    "train_label = train_data[\"label\"].copy(deep=True)\n",
    "valid_label = valid_data[\"label\"].copy(deep=True)\n",
    "test_label = test_data[\"label\"].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "3F7WlKYeVJjc"
   },
   "outputs": [],
   "source": [
    "train_data = pad_input(train_data[\"issue\"].values)\n",
    "valid_data = pad_input(valid_data[\"issue\"].values)\n",
    "test_data = pad_input(test_data[\"issue\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "XI7fKLiIULCj"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_data).to(device), torch.from_numpy(train_label.values).to(device))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_data).to(device), torch.from_numpy(valid_label.values).to(device))\n",
    "test_data = TensorDataset(torch.from_numpy(test_data).to(device), torch.from_numpy(test_label.values).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "9hgk40EcJHeE"
   },
   "outputs": [],
   "source": [
    "num_class = len(dataset[\"label\"].unique())\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "hidden_dim = 512\n",
    "model = LSTMClassifier(vocab_size, emsize, hidden_dim, num_class, n_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeFrcoecK702",
    "outputId": "bd6c210a-5290-44bc-e618-d7f4ff620ff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    50/  247 batches | accuracy    0.761 | time 2.993\n",
      "| epoch   1 |   100/  247 batches | accuracy    0.823 | time 2.855\n",
      "| epoch   1 |   150/  247 batches | accuracy    0.798 | time 2.862\n",
      "| epoch   1 |   200/  247 batches | accuracy    0.782 | time 2.915\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.60s | valid accuracy    0.641 \n",
      "-----------------------------------------------------------------\n",
      "| epoch   2 |    50/  247 batches | accuracy    0.871 | time 3.096\n",
      "| epoch   2 |   100/  247 batches | accuracy    0.866 | time 3.053\n",
      "| epoch   2 |   150/  247 batches | accuracy    0.868 | time 3.050\n",
      "| epoch   2 |   200/  247 batches | accuracy    0.865 | time 3.048\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   2 | time: 15.28s | valid accuracy    0.669 \n",
      "-----------------------------------------------------------------\n",
      "| epoch   3 |    50/  247 batches | accuracy    0.929 | time 3.092\n",
      "| epoch   3 |   100/  247 batches | accuracy    0.910 | time 3.047\n",
      "| epoch   3 |   150/  247 batches | accuracy    0.896 | time 3.047\n",
      "| epoch   3 |   200/  247 batches | accuracy    0.893 | time 3.047\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   3 | time: 15.25s | valid accuracy    0.670 \n",
      "-----------------------------------------------------------------\n",
      "| epoch   4 |    50/  247 batches | accuracy    0.934 | time 3.045\n",
      "| epoch   4 |   100/  247 batches | accuracy    0.928 | time 2.982\n",
      "| epoch   4 |   150/  247 batches | accuracy    0.920 | time 2.950\n",
      "| epoch   4 |   200/  247 batches | accuracy    0.916 | time 2.952\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   4 | time: 14.86s | valid accuracy    0.667 \n",
      "-----------------------------------------------------------------\n",
      "| epoch   5 |    50/  247 batches | accuracy    0.951 | time 2.971\n",
      "| epoch   5 |   100/  247 batches | accuracy    0.947 | time 2.908\n",
      "| epoch   5 |   150/  247 batches | accuracy    0.943 | time 2.918\n",
      "| epoch   5 |   200/  247 batches | accuracy    0.936 | time 2.902\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   5 | time: 14.61s | valid accuracy    0.652 \n",
      "-----------------------------------------------------------------\n",
      "| epoch   6 |    50/  247 batches | accuracy    0.949 | time 3.019\n",
      "| epoch   6 |   100/  247 batches | accuracy    0.950 | time 2.975\n",
      "| epoch   6 |   150/  247 batches | accuracy    0.947 | time 3.002\n",
      "| epoch   6 |   200/  247 batches | accuracy    0.945 | time 2.978\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   6 | time: 14.96s | valid accuracy    0.657 \n",
      "-----------------------------------------------------------------\n",
      "| epoch   7 |    50/  247 batches | accuracy    0.961 | time 3.024\n",
      "| epoch   7 |   100/  247 batches | accuracy    0.948 | time 2.955\n",
      "| epoch   7 |   150/  247 batches | accuracy    0.947 | time 2.958\n",
      "| epoch   7 |   200/  247 batches | accuracy    0.952 | time 2.993\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   7 | time: 14.93s | valid accuracy    0.675 \n",
      "-----------------------------------------------------------------\n",
      "| epoch   8 |    50/  247 batches | accuracy    0.953 | time 3.015\n",
      "| epoch   8 |   100/  247 batches | accuracy    0.962 | time 2.965\n",
      "| epoch   8 |   150/  247 batches | accuracy    0.959 | time 2.966\n",
      "| epoch   8 |   200/  247 batches | accuracy    0.951 | time 2.950\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   8 | time: 14.85s | valid accuracy    0.664 \n",
      "-----------------------------------------------------------------\n",
      "| epoch   9 |    50/  247 batches | accuracy    0.965 | time 3.074\n",
      "| epoch   9 |   100/  247 batches | accuracy    0.957 | time 3.007\n",
      "| epoch   9 |   150/  247 batches | accuracy    0.951 | time 3.012\n",
      "| epoch   9 |   200/  247 batches | accuracy    0.954 | time 3.007\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch   9 | time: 15.08s | valid accuracy    0.669 \n",
      "-----------------------------------------------------------------\n",
      "| epoch  10 |    50/  247 batches | accuracy    0.963 | time 3.019\n",
      "| epoch  10 |   100/  247 batches | accuracy    0.961 | time 2.964\n",
      "| epoch  10 |   150/  247 batches | accuracy    0.963 | time 2.963\n",
      "| epoch  10 |   200/  247 batches | accuracy    0.948 | time 2.985\n",
      "-----------------------------------------------------------------\n",
      "| end of epoch  10 | time: 14.90s | valid accuracy    0.639 \n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "LR = 0.01  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "total_accu = None\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    h = model.init_hidden(BATCH_SIZE)\n",
    "    train(train_dataloader, criterion, h)\n",
    "    h = model.init_hidden(BATCH_SIZE)\n",
    "    accu_val = evaluate(valid_dataloader, criterion, h)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 65)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Md8eb07ZXQUK",
    "outputId": "6926d538-f7b1-4fb4-f96c-cd9990fb4c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    63.64\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader, criterion, model.init_hidden(BATCH_SIZE))\n",
    "print('test accuracy {:8.2f}'.format(accu_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "KX2X1Yc1fTRB"
   },
   "outputs": [],
   "source": [
    "torch.save(model, '/content/drive/MyDrive/github-issue-bot/models/lstm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47uX8LjllwD8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
